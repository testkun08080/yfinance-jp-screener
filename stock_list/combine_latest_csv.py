#!/usr/bin/env python3
"""
Latest CSV Combiner for Japanese Stock Data
æœ€æ–°ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’çµåˆã—ã¦æ—¥ä»˜ä»˜ãã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

Purpose:
- Exportãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰æœ€æ–°ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç‰¹å®š
- è¤‡æ•°ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’çµåˆã—ã¦ä¸€ã¤ã®çµ±åˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
- æ—¥ä»˜_combined.csvå½¢å¼ã§ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
"""

import os
import glob
import pandas as pd
from datetime import datetime
import argparse
import logging
from pathlib import Path

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler("combine_csv.log", encoding="utf-8"),
        logging.StreamHandler(),
    ],
)
logger = logging.getLogger(__name__)


def get_latest_csv_files(export_dir="./Export", target_date=None):
    """
    Exportãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰æœ€æ–°ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—

    Args:
        export_dir (str): CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒæ ¼ç´ã•ã‚Œã¦ã„ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        target_date (str): å¯¾è±¡æ—¥ä»˜ (YYYYMMDDå½¢å¼ã€Noneã®å ´åˆã¯ä»Šæ—¥)

    Returns:
        list: æœ€æ–°ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªã‚¹ãƒˆï¼ˆä»Šæ—¥ã®æ—¥ä»˜ã®ã‚‚ã®ã®ã¿ï¼‰
    """
    # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å®šç¾©
    pattern = os.path.join(export_dir, "japanese_stocks_data_*.csv")
    all_csv_files = glob.glob(pattern)

    if not all_csv_files:
        logger.warning(f"CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {pattern}")
        return []

    # å¯¾è±¡æ—¥ä»˜ã‚’æ±ºå®š
    if target_date is None:
        target_date = datetime.now().strftime("%Y%m%d")

    logger.info(f"å¯¾è±¡æ—¥ä»˜: {target_date}")

    # ä»Šæ—¥ã®æ—¥ä»˜ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    csv_files = [f for f in all_csv_files if target_date in os.path.basename(f)]

    if not csv_files:
        logger.warning(f"âš ï¸  {target_date} ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        logger.info(f"å…¨{len(all_csv_files)}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ¤œç´¢ã—ã¾ã—ãŸãŒã€è©²å½“ãªã—")
        return []

    # ãƒ•ã‚¡ã‚¤ãƒ«ã®æ›´æ–°æ—¥æ™‚ã§ã‚½ãƒ¼ãƒˆï¼ˆæœ€æ–°é †ï¼‰
    csv_files.sort(key=os.path.getmtime, reverse=True)

    # å„ãƒ•ã‚¡ã‚¤ãƒ«ã®æƒ…å ±ã‚’ãƒ­ã‚°å‡ºåŠ›
    logger.info(f"âœ… {target_date} ã®CSVãƒ•ã‚¡ã‚¤ãƒ«: {len(csv_files)}å€‹")
    for i, file in enumerate(csv_files):
        mod_time = datetime.fromtimestamp(os.path.getmtime(file))
        logger.info(f"  {i + 1}. {os.path.basename(file)} (æ›´æ–°æ—¥æ™‚: {mod_time})")

    return csv_files


def get_today_date():
    """
    ä»Šæ—¥ã®æ—¥ä»˜ã‚’YYYYMMDDå½¢å¼ã§å–å¾—

    Returns:
        str: ä»Šæ—¥ã®æ—¥ä»˜ï¼ˆYYYYMMDDå½¢å¼ï¼‰
    """
    return datetime.now().strftime("%Y%m%d")


def combine_csv_files(csv_files, output_file):
    """
    è¤‡æ•°ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’çµåˆã—ã¦ä¸€ã¤ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜

    Args:
        csv_files (list): çµåˆã™ã‚‹CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªã‚¹ãƒˆ
        output_file (str): å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«å

    Returns:
        bool: æˆåŠŸã—ãŸå ´åˆTrueã€å¤±æ•—ã—ãŸå ´åˆFalse
    """
    try:
        combined_data = []
        total_rows = 0

        for csv_file in csv_files:
            logger.info(f"èª­ã¿è¾¼ã¿ä¸­: {os.path.basename(csv_file)}")

            # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ï¼ˆæ—¥æœ¬èªå¯¾å¿œï¼‰
            df = pd.read_csv(csv_file, encoding="utf-8")

            # BOMï¼ˆByte Order Markï¼‰ã‚’é™¤å»
            if df.columns[0].startswith("\ufeff"):
                df.columns = [df.columns[0].replace("\ufeff", "")] + df.columns[
                    1:
                ].tolist()

            # ãƒ‡ãƒ¼ã‚¿ã®åŸºæœ¬æƒ…å ±ã‚’ãƒ­ã‚°å‡ºåŠ›
            logger.info(f"  - è¡Œæ•°: {len(df)}, åˆ—æ•°: {len(df.columns)}")

            combined_data.append(df)
            total_rows += len(df)

        if not combined_data:
            logger.error("çµåˆã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return False

        # ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆï¼ˆé‡è¤‡æ’é™¤ï¼‰
        logger.info("CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’çµåˆä¸­...")
        combined_df = pd.concat(combined_data, ignore_index=True)

        # é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã®é™¤å»ï¼ˆéŠ˜æŸ„ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ï¼‰
        if "éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰" in combined_df.columns:
            before_dedup = len(combined_df)
            combined_df = combined_df.drop_duplicates(
                subset=["éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰"], keep="last"
            )
            after_dedup = len(combined_df)
            logger.info(
                f"é‡è¤‡é™¤å»: {before_dedup} â†’ {after_dedup} è¡Œ ({before_dedup - after_dedup}è¡Œã‚’é™¤å»)"
            )

        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        os.makedirs(os.path.dirname(output_file), exist_ok=True)

        # çµåˆã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
        combined_df.to_csv(output_file, index=False, encoding="utf-8")

        logger.info(f"âœ… çµåˆå®Œäº†: {output_file}")
        logger.info(f"   - ç·è¡Œæ•°: {len(combined_df)}")
        logger.info(f"   - ç·åˆ—æ•°: {len(combined_df.columns)}")
        logger.info(
            f"   - ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {os.path.getsize(output_file) / (1024 * 1024):.2f} MB"
        )

        return True

    except Exception as e:
        logger.error(f"âŒ CSVãƒ•ã‚¡ã‚¤ãƒ«çµåˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {str(e)}")
        return False


def main():
    """
    ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°
    """
    parser = argparse.ArgumentParser(
        description="æœ€æ–°ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’çµåˆã—ã¦æ—¥ä»˜ä»˜ããƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆ"
    )
    parser.add_argument(
        "--export-dir",
        default="./Export",
        help="CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒæ ¼ç´ã•ã‚Œã¦ã„ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ./Export)",
    )
    parser.add_argument(
        "--output-dir",
        default="./Export",
        help="å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ./Export)",
    )
    parser.add_argument(
        "--date",
        default=None,
        help="ä½¿ç”¨ã™ã‚‹æ—¥ä»˜ (YYYYMMDDå½¢å¼ã€æœªæŒ‡å®šã®å ´åˆã¯ä»Šæ—¥ã®æ—¥ä»˜)",
    )

    args = parser.parse_args()

    # å®Ÿè¡Œé–‹å§‹ãƒ­ã‚°
    logger.info("=" * 60)
    logger.info("ğŸš€ Latest CSV Combiner å®Ÿè¡Œé–‹å§‹")
    logger.info("=" * 60)

    # å¯¾è±¡æ—¥ä»˜ã‚’æ±ºå®š
    target_date = args.date if args.date else get_today_date()

    # æŒ‡å®šæ—¥ä»˜ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
    csv_files = get_latest_csv_files(args.export_dir, target_date)

    if not csv_files:
        logger.error(f"âŒ {target_date} ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return False

    # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
    output_filename = f"{target_date}_combined.csv"
    output_path = os.path.join(args.output_dir, output_filename)

    logger.info(f"ğŸ“ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {output_path}")

    # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’çµåˆ
    success = combine_csv_files(csv_files, output_path)

    if success:
        logger.info("=" * 60)
        logger.info("âœ… CSVçµåˆå‡¦ç†ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ")
        logger.info("=" * 60)
        print(f"OUTPUT_FILE={output_path}")  # GitHub Actionsç”¨ã®å‡ºåŠ›
        return True
    else:
        logger.error("=" * 60)
        logger.error("âŒ CSVçµåˆå‡¦ç†ãŒå¤±æ•—ã—ã¾ã—ãŸ")
        logger.error("=" * 60)
        return False


if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)
