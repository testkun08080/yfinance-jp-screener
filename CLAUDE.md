# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Repository Overview

**å€‹äººå‘ã‘æ—¥æœ¬æ ªå¼åˆ†æãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ** - yfinance APIã‚’æ´»ç”¨ã—ãŸ3795+éŠ˜æŸ„ã®å°å‹æ ªåˆ†æãƒ„ãƒ¼ãƒ«

ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã¯å€‹äººé–‹ç™ºãƒ»å€‹äººç’°å¢ƒã§ã®ä½¿ç”¨ã‚’æƒ³å®šã—ãŸã€ã‚·ãƒ³ãƒ—ãƒ«ã§åŠ¹ç‡çš„ãªæ ªå¼åˆ†æã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚

### ã‚·ã‚¹ãƒ†ãƒ æ§‹æˆ

**1. ãƒ‡ãƒ¼ã‚¿åé›†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³** (Python 3.11)
- JPXå…¬å¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æœ€æ–°æ ªå¼ãƒªã‚¹ãƒˆè‡ªå‹•å–å¾—
- yfinance APIã«ã‚ˆã‚‹è²¡å‹™ãƒ‡ãƒ¼ã‚¿åé›†
- 4æ®µéšSequentialå®Ÿè¡Œã§APIåˆ¶é™ãƒ»ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå›é¿
- è‡ªå‹•CSVçµåˆæ©Ÿèƒ½

**2. Webã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³** (React 19 + TypeScript + Vite)
- ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ­ãƒ¼ã‚«ãƒ«ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆSEOãƒ»PWAå‰Šé™¤æ¸ˆã¿ï¼‰
- å‹•çš„ã‚«ãƒ©ãƒ æ¤œå‡ºãƒ»æ—¥æœ¬èªé‡‘èãƒ‡ãƒ¼ã‚¿å¯¾å¿œ
- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¤œç´¢ãƒ»ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
- ãƒ¬ã‚¹ãƒãƒ³ã‚·ãƒ–ãƒ‡ã‚¶ã‚¤ãƒ³ï¼ˆTailwind CSS + DaisyUIï¼‰

**3. Dockerç’°å¢ƒ** (nginx + Python)
- æœ¬ç•ªåŒç­‰ã®å®Ÿè¡Œç’°å¢ƒ
- ãƒãƒ«ãƒã‚¹ãƒ†ãƒ¼ã‚¸ãƒ“ãƒ«ãƒ‰æœ€é©åŒ–
- ãƒœãƒªãƒ¥ãƒ¼ãƒ å…±æœ‰ã«ã‚ˆã‚‹ãƒ‡ãƒ¼ã‚¿é€£æº
- nginxã«ã‚ˆã‚‹é«˜é€Ÿé™çš„ãƒ•ã‚¡ã‚¤ãƒ«é…ä¿¡

**4. GitHub Actions CI** (7ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼)
- Sequential Stock Fetch (Part 1-4): å„120åˆ†å®Ÿè¡Œ
- CSV Combine & Export
- Stock List Update
- Stock Data Fetch (å˜ä½“ãƒ†ã‚¹ãƒˆç”¨)

### é‡è¦ãªè¨­è¨ˆæ–¹é‡

âœ… **å€‹äººç’°å¢ƒç‰¹åŒ–**
- SEOæœ€é©åŒ–ä¸è¦ï¼ˆå‰Šé™¤æ¸ˆã¿ï¼‰
- PWAæ©Ÿèƒ½ä¸è¦ï¼ˆå‰Šé™¤æ¸ˆã¿ï¼‰
- GitHub Pagesä¸è¦ï¼ˆå‰Šé™¤æ¸ˆã¿ï¼‰
- ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ­ãƒ¼ã‚«ãƒ«/Dockerå®Ÿè¡Œ

âœ… **è‡ªå‹•åŒ–é‡è¦–**
- ãƒ‡ãƒ¼ã‚¿åé›†ã®å®Œå…¨è‡ªå‹•åŒ–
- ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ»ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½
- ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼é€£é–å®Ÿè¡Œ

âœ… **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–**
- ãƒ™ãƒ³ãƒ€ãƒ¼ãƒãƒ£ãƒ³ã‚¯åˆ†é›¢
- nginxé™çš„é…ä¿¡
- Docker build cacheæ´»ç”¨

### ğŸ“š è©³ç´°ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

å„ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®è©³ç´°ãªèª¬æ˜ã¯ã€ãã‚Œãã‚Œã®READMEã‚’å‚ç…§ã—ã¦ãã ã•ã„ï¼š

- **[stock_list/README.md](stock_list/README.md)** - ãƒ‡ãƒ¼ã‚¿åé›†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®è©³ç´°ï¼ˆPython scripts, CSV processingï¼‰
- **[stock_search/README.md](stock_search/README.md)** - React Webã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®è©³ç´°ï¼ˆFrontend, build processï¼‰
- **[README.md](README.md)** - ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå…¨ä½“ã®æ¦‚è¦ã¨ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

## Repository Structure

```
yfinance-jp-screener/
â”œâ”€â”€ .github/workflows/                    # GitHub Actions automation (7 workflows)
â”‚   â”œâ”€â”€ stock-data-fetch.yml             # Manual: Single stock file processing
â”‚   â”œâ”€â”€ stock-fetch-sequential-1.yml     # Automated: Part 1/4 â†’ triggers Part 2
â”‚   â”œâ”€â”€ stock-fetch-sequential-2.yml     # Automated: Part 2/4 â†’ triggers Part 3
â”‚   â”œâ”€â”€ stock-fetch-sequential-3.yml     # Automated: Part 3/4 â†’ triggers Part 4
â”‚   â”œâ”€â”€ stock-fetch-sequential-4.yml     # Automated: Part 4/4 â†’ triggers CSV combine
â”‚   â”œâ”€â”€ csv-combine-export.yml           # Automated: Combine CSVs
â”‚   â””â”€â”€ stock-list-update.yml            # Manual: Stock list updates and splitting
â”œâ”€â”€ current/                              # Personal portfolio data
â”‚   â””â”€â”€ tes.md                           # Portfolio tracking file
â”œâ”€â”€ stock_list/                           # Data collection and processing
â”‚   â”œâ”€â”€ Export/                          # Generated CSV data exports (gitignored in production)
â”‚   â”œâ”€â”€ combine_latest_csv.py            # CSV combination script
â”‚   â”œâ”€â”€ sumalize.py                      # Main data collection script with yfinance
â”‚   â”œâ”€â”€ split_stocks.py                  # JSON file splitting utility (enhanced CLI)
â”‚   â”œâ”€â”€ get_jp_stocklist.py              # Stock list acquisition from JPX
â”‚   â”œâ”€â”€ stocks_all.json                  # Master stock list (~3795 companies)
â”‚   â”œâ”€â”€ stocks_1.json                    # Split 1: Companies 1-1000
â”‚   â”œâ”€â”€ stocks_2.json                    # Split 2: Companies 1001-2000
â”‚   â”œâ”€â”€ stocks_3.json                    # Split 3: Companies 2001-3000
â”‚   â”œâ”€â”€ stocks_4.json                    # Split 4: Companies 3001-3795
â”‚   â”œâ”€â”€ requirements.txt                 # Python dependencies (pandas, yfinance, etc.)
â”‚   â””â”€â”€ pyproject.toml                   # Python project configuration
â”œâ”€â”€ search/                                # Research and analysis data
â”‚   â”œâ”€â”€ kogata/                          # Small-cap company data by industry
â”‚   â”‚   â”œâ”€â”€ *.csv                        # Industry-specific financial data (23 sectors)
â”‚   â”œâ”€â”€ high_netcash/                    # High net cash companies analysis
â”‚   â”‚   â”œâ”€â”€ *.csv                        # Sector-specific high net cash data
â”‚   â”‚   â”œâ”€â”€ summary_data.json            # Analysis summary
â”‚   â”‚   â””â”€â”€ é«˜ãƒãƒƒãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥æ¯”ç‡ä¼æ¥­åˆ†æ.md
â”‚   â””â”€â”€ çµ±åˆãƒ‡ãƒ¼ã‚¿*.csv                   # Consolidated datasets
â”œâ”€â”€ stock_search/                         # Web application (React + TypeScript + Vite)
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/                  # React components
â”‚   â”‚   â”‚   â”œâ”€â”€ DataTable.tsx           # Dynamic CSV display
â”‚   â”‚   â”‚   â”œâ”€â”€ FileUpload.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ SearchFilters.tsx
â”‚   â”‚   â”‚   â””â”€â”€ Pagination.tsx
â”‚   â”‚   â”œâ”€â”€ hooks/                       # Custom React hooks
â”‚   â”‚   â”‚   â”œâ”€â”€ useCSVData.ts
â”‚   â”‚   â”‚   â”œâ”€â”€ useCSVParser.ts
â”‚   â”‚   â”‚   â””â”€â”€ useFilters.ts
â”‚   â”‚   â”œâ”€â”€ utils/                       # Utility functions
â”‚   â”‚   â”‚   â”œâ”€â”€ csvParser.ts            # CSV parsing and formatting
â”‚   â”‚   â”‚   â””â”€â”€ csvDownload.ts          # CSV download utilities
â”‚   â”‚   â”œâ”€â”€ types/                       # TypeScript definitions
â”‚   â”‚   â”‚   â””â”€â”€ stock.ts
â”‚   â”‚   â””â”€â”€ App.tsx                      # Main application
â”‚   â”œâ”€â”€ public/                          # Public assets
â”‚   â”‚   â”œâ”€â”€ csv/                         # CSV files (not in git, copied during build)
â”‚   â”‚   â””â”€â”€ favicon.ico                  # Favicon
â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â””â”€â”€ copy-csv-files.js            # Prebuild script to copy CSV files
â”‚   â”œâ”€â”€ dist/                            # Built application
â”‚   â”‚   â””â”€â”€ csv/                         # CSV files included in build
â”‚   â”œâ”€â”€ nginx.conf                       # Nginx configuration for Docker production
â”‚   â”œâ”€â”€ package.json                     # Node.js dependencies
â”‚   â”œâ”€â”€ vite.config.ts                   # Vite configuration
â”‚   â””â”€â”€ tsconfig.json                    # TypeScript configuration
â”œâ”€â”€ Dockerfile.fetch                     # Python service (data collection)
â”œâ”€â”€ Dockerfile.app                   # Frontend service (nginx production)
â”œâ”€â”€ docker-compose.yml                    # Docker orchestration configuration
â”œâ”€â”€ .env.sample                          # Environment variables template
â”œâ”€â”€ CLAUDE.md                            # This file
â”œâ”€â”€ DOCKER.md                            # Docker deployment documentation
â””â”€â”€ README.md                            # Project documentation
```

## System Components

### 1. Data Collection Pipeline (`stock_list/`)

**Core Scripts:**
- `sumalize.py` - Main data collection script with yfinance API integration
- `split_stocks.py` - Enhanced utility with command-line arguments for flexible file splitting
- `get_jp_stocklist.py` - Stock list acquisition from JPX official data sources
- `combine_latest_csv.py` - Combines multiple CSV exports into single dated file

**Data Processing Features:**
- **Enhanced CLI**: `split_stocks.py` supports `--input`, `--size`, and `--verbose` flags
- Flexible input file specification (default: `stocks_all.json`)
- Customizable chunk sizes for different use cases
- Comprehensive logging with execution time tracking
- Error handling and retry mechanisms for API failures
- Rate limiting for API compliance and stability
- Automatic CSV generation with timestamping
- Master stock list management (`stocks_all.json`)

**Usage Examples:**
```bash
# Process default file
python sumalize.py

# Process specific chunk
python sumalize.py stocks_1.json

# Update master stock list
python get_jp_stocklist.py

# Enhanced split functionality with arguments
python split_stocks.py --input stocks_all.json --size 1000
python split_stocks.py -i custom_data.json -s 500 --verbose

# Combine latest CSV files
python combine_latest_csv.py
python combine_latest_csv.py --date 20251006
```

### 2. Web Application (`stock_search/`)

**Technology Stack:**
- **Frontend**: React 19 + TypeScript + Vite
- **Styling**: Tailwind CSS + DaisyUI
- **State Management**: Custom hooks with local state
- **CSV Processing**: Papa Parse with Japanese character support
- **Deployment**: Docker with nginx

**Key Features:**
- Dynamic column detection and display for any CSV structure
- Japanese financial data formatting (å††, %, å€)
- Real-time search and filtering capabilities
- Responsive design with sticky columns for mobile compatibility
- Pagination and sorting capabilities
- File upload with drag-and-drop support
- Optimized bundle splitting for performance
- CSV files automatically copied during build via prebuild script

**Build Process:**
1. **Prebuild**: `node scripts/copy-csv-files.js` copies latest combined CSV from `stock_list/Export/` to `public/csv/`
2. **Build**: Vite builds application with optimized vendor chunks
3. **Output**: `dist/` directory with CSV files included at `dist/csv/`

**Docker Deployment:**
- **Container**: nginx:alpine serving static build
- **Volume**: CSV files mounted from shared volume
- **Performance**: Vendor chunks separated for efficient caching
- **Access**: http://localhost:8080

### 3. Docker Environment

**Two-Service Architecture:**

#### **Python Service** (`Dockerfile.fetch`)
- **Base Image**: python:3.11-slim
- **Purpose**: Data collection and processing
- **Working Directory**: `/app`
- **Volumes**: `stock-data` volume shared at `/app/Export`
- **Default Command**: Sequential execution of:
  1. `get_jp_stocklist.py` - Fetch latest stock list
  2. `split_stocks.py` - Split into chunks
  3. `sumalize.py` - Collect financial data
  4. `combine_latest_csv.py` - Combine CSV files
- **Environment Variables**:
  - `STOCK_FILE` - Stock file to process (default: stocks_1.json)
  - `CHUNK_SIZE` - Split chunk size (default: 1000)

#### **Frontend Service** (`Dockerfile.app`)
- **Base Image**: nginx:alpine (production)
- **Multi-stage Build**:
  1. **Builder Stage**: Node.js 20 - builds React application
  2. **Runner Stage**: nginx - serves static files
- **Working Directory**: `/usr/share/nginx/html`
- **Volumes**: `stock-data` volume mounted at `/usr/share/nginx/html/csv` (read-only)
- **Port**: 80 (exposed as 8080 on host)
- **Configuration**: Custom nginx.conf with:
  - SPA routing support
  - Gzip compression
  - Static asset caching
  - CSV file CORS headers
  - Security headers

**Data Flow:**
```
Python Container â†’ /app/Export (combine_latest_csv.py)
       â†“
  stock-data volume
       â†“
Frontend Container â†’ /usr/share/nginx/html/csv (nginx serving)
       â†“
  Browser access: http://localhost:8080/csv/YYYYMMDD_combined.csv
```

**Docker Compose Configuration:**
- **Network**: `stock-network` (bridge driver)
- **Volume**: `stock-data` (local driver) - shared between services
- **Dependencies**: Frontend depends on Python service completion
- **Health Checks**: Frontend HTTP health check on port 80
- **Restart Policy**: Python service runs once, Frontend always available

**Usage:**
```bash
# Start both services
docker-compose up --build

# Access frontend
open http://localhost:8080

# View logs
docker-compose logs -f

# Stop services
docker-compose down

# Clean volumes
docker-compose down -v
```

### 4. GitHub Actions Automation (`.github/workflows/`)

**Workflow Orchestration Chain:**

```
Sequential Data Collection Workflows (Manual Start)
    â†“
stock-fetch-sequential-1.yml (Part 1/4)
    â†“ Auto-trigger on success
stock-fetch-sequential-2.yml (Part 2/4)
    â†“ Auto-trigger on success
stock-fetch-sequential-3.yml (Part 3/4)
    â†“ Auto-trigger on success
stock-fetch-sequential-4.yml (Part 4/4)
    â†“ Auto-trigger on success
csv-combine-export.yml (Combine all CSV files)
    â†“
CSV Data Ready for Local/Docker Use
```

#### **Workflow 1: `stock-data-fetch.yml` (Manual Single File Processing)**
- **Trigger**: Manual (workflow_dispatch)
- **Purpose**: Process single stock file for quick testing
- **Input**: Stock file selection (stocks_1.json - stocks_4.json, stocks_sample.json)
- **Timeout**: 60 minutes
- **Process**: Data collection â†’ Export to stock_list/Export/ â†’ Commit
- **Use Case**: Quick data collection for specific stock chunk

#### **Workflow 2-5: Sequential Stock Fetch Workflows**
**Part 1** (`stock-fetch-sequential-1.yml`):
- **Trigger**: Manual (workflow_dispatch)
- **Process**: stocks_1.json â†’ Commit â†’ Auto-trigger Part 2
- **Timeout**: 120 minutes

**Part 2** (`stock-fetch-sequential-2.yml`):
- **Trigger**: Auto (triggered by Part 1 completion)
- **Process**: stocks_2.json â†’ Commit â†’ Auto-trigger Part 3

**Part 3** (`stock-fetch-sequential-3.yml`):
- **Trigger**: Auto (triggered by Part 2 completion)
- **Process**: stocks_3.json â†’ Commit â†’ Auto-trigger Part 4

**Part 4 (Final)** (`stock-fetch-sequential-4.yml`):
- **Trigger**: Auto (triggered by Part 3 completion)
- **Process**: stocks_4.json â†’ Commit â†’ Auto-trigger CSV Combine
- **Special**: Completion summary and workflow chain trigger

**Why Sequential?**
- Avoids API rate limiting from yfinance
- Prevents GitHub Actions timeout (max 6 hours total, 120 min per workflow)
- Allows monitoring and intervention at each stage
- Commits data incrementally for safety

#### **Workflow 6: `csv-combine-export.yml` (CSV Combination)**
- **Trigger**:
  - Auto (triggered by Sequential Part 4 completion)
  - Manual (workflow_dispatch)
- **Purpose**: Combine all japanese_stocks_data_*.csv files into single dated file
- **Input Parameters** (manual only):
  - `custom_date` - Custom date for output filename (YYYYMMDD)
  - `reason` - Reason for combination
- **Process**:
  1. Check existing CSV files in stock_list/Export/
  2. Run `combine_latest_csv.py` with arguments
  3. Commit combined file (YYYYMMDD_combined.csv)
- **Output**: `YYYYMMDD_combined.csv` in stock_list/Export/

#### **Workflow 7: `stock-list-update.yml` (Master List Update)**
- **Trigger**: Manual (workflow_dispatch)
- **Purpose**: Update master stock list from JPX
- **Process**:
  1. Run `get_jp_stocklist.py` to fetch latest JPX data
  2. Generate `stocks_all.json` (~3795 companies)
  3. Split using `split_stocks.py --input stocks_all.json --size 1000`
  4. Validate all generated JSON files
  5. Commit with Japanese date format
- **Output**: Updated stocks_all.json and stocks_1-4.json
- **Frequency**: Run when JPX publishes new stock listings (monthly/quarterly)

### 5. CSV Data Flow Architecture

**Two Deployment Contexts:**

#### **A. Local Development**
```
Run python scripts locally â†’ stock_list/Export/
    â†“
cd stock_search && npm run build
    â†“
prebuild: copy-csv-files.js
    â†“
Copy ../stock_list/Export/*_combined.csv â†’ public/csv/
    â†“
Vite build â†’ dist/csv/
    â†“
npm run preview â†’ http://localhost:4173 (Vite preview)
or
docker-compose up â†’ http://localhost:8080 (nginx production)
```

#### **B. Docker Environment**
```
Python Container:
  sumalize.py â†’ /app/Export/japanese_stocks_data_*.csv
  combine_latest_csv.py â†’ /app/Export/YYYYMMDD_combined.csv
    â†“
  stock-data volume (/app/Export)
    â†“
Frontend Container:
  nginx serves /usr/share/nginx/html (dist/ copied during build)
  stock-data volume mounted at /usr/share/nginx/html/csv (read-only)
    â†“
  Browser: http://localhost:8080/csv/YYYYMMDD_combined.csv
```

**Key Insights**:
- Local development: CSV files included in build via prebuild script
- Docker deployment: Volume mounting for dynamic CSV access
- Vite preview server: Port 4173 (development testing)
- nginx production server: Port 8080 (Docker environment)
- Consistent `/csv/` path across all deployment contexts

## Data Architecture

### Stock List Management

**Master Data Source**: JPX (Japan Exchange Group) official data
- **Source**: `https://www.jpx.co.jp/markets/statistics-equities/misc/tvdivq0000001vg2-att/data_j.xls`
- **Format**: Excel â†’ JSON conversion with Japanese character support
- **Markets**: ãƒ—ãƒ©ã‚¤ãƒ  (Prime), ã‚¹ã‚¿ãƒ³ãƒ€ãƒ¼ãƒ‰ (Standard), ã‚°ãƒ­ãƒ¼ã‚¹ (Growth)
- **Data Fields**: ã‚³ãƒ¼ãƒ‰ (Stock Code), éŠ˜æŸ„å (Company Name), å¸‚å ´ãƒ»å•†å“åŒºåˆ† (Market Classification), 33æ¥­ç¨®åŒºåˆ† (Industry Classification)

**File Structure:**
```
stocks_all.json      # Master list (~3795 companies)
â”œâ”€â”€ stocks_1.json    # Companies 1-1000
â”œâ”€â”€ stocks_2.json    # Companies 1001-2000
â”œâ”€â”€ stocks_3.json    # Companies 2001-3000
â””â”€â”€ stocks_4.json    # Companies 3001-3795
```

### Industry Data Structure (search/kogata/)

**Core Financial Fields:**
- **Basic Info**: ä¼šç¤¾å (Company Name), éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰ (Stock Code), æ¥­ç¨® (Industry)
- **Market Data**: å„ªå…ˆå¸‚å ´ (Preferred Market), æ™‚ä¾¡ç·é¡ (Market Cap), æ±ºç®—æœˆ (Settlement Month)
- **Geographic**: éƒ½é“åºœçœŒ (Prefecture), ä¼šè¨ˆåŸºæº– (Accounting Standard)
- **Valuation Ratios**: PBR, ROE, è‡ªå·±è³‡æœ¬æ¯”ç‡ (Equity Ratio), PER(ä¼šäºˆ) (Projected PER)
- **Performance Metrics**: å£²ä¸Šé«˜ (Revenue), å–¶æ¥­åˆ©ç›Š (Operating Profit), å–¶æ¥­åˆ©ç›Šç‡ (Operating Margin)
- **Profitability**: å½“æœŸç´”åˆ©ç›Š (Net Profit), ç´”åˆ©ç›Šç‡ (Net Margin)
- **Balance Sheet**: è² å‚µ (Debt), æµå‹•è² å‚µ (Current Liabilities), æµå‹•è³‡ç”£ (Current Assets)
- **Cash Analysis**: ãƒãƒƒãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆæµå‹•è³‡ç”£-è² å‚µï¼‰, ãƒãƒƒãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥æ¯”ç‡
- **Additional Fields**: ç·è² å‚µ, ç¾é‡‘åŠã³ç¾é‡‘åŒç­‰ç‰©, æŠ•è³‡æœ‰ä¾¡è¨¼åˆ¸

**Data Quality Features:**
- Automatic data type detection and conversion
- Japanese unit parsing (å€, %, å††) with proper formatting
- Null value handling and validation
- Unicode support for Japanese characters
- Flexible schema adaptation for varying CSV structures

**Industry Sectors Available (23 sectors):**
- **Basic Industries**: åŒ–å­¦, é‰„é‹¼, éé‰„é‡‘å±, é‡‘å±è£½å“, ã‚´ãƒ 
- **Manufacturing**: æ©Ÿæ¢°, é›»æ°—æ©Ÿå™¨, é‹é€ç”¨, æ©Ÿå¯†è£½å“, ãƒ‘ãƒ«ãƒ—ãƒ»ç´™, ç¹Šç¶­
- **Services**: ã‚µãƒ¼ãƒ“ã‚¹, æƒ…å ±ãƒ»é€šä¿¡æ¥­, å€‰åº«ãƒ»é‹è¼¸é–¢é€£æ¥­
- **Consumer**: å°å£²æ¥­, å¸æ¥­è€…, é£Ÿæ–™å“
- **Real Estate & Finance**: ä¸å‹•ç”£æ¥­ç•Œ, è¨¼åˆ¸
- **Resources**: æ°´ç”£ãƒ»è¾²æ—æ¥­, çŸ³æ²¹ãƒ»çŸ³ç‚­è£½å“
- **Construction**: å»ºè¨­æ¥­ç•Œ
- **Other**: ãã®ä»–

## Development Context

This repository serves as a comprehensive Japanese stock analysis platform with:

### Japanese Business Focus
- **Language**: Primary data and interface in Japanese with Unicode support
- **Market**: Japanese stock exchanges (ãƒ—ãƒ©ã‚¤ãƒ , ã‚¹ã‚¿ãƒ³ãƒ€ãƒ¼ãƒ‰, ã‚°ãƒ­ãƒ¼ã‚¹)
- **Specialization**: Small-cap company (å°å‹æ ª) analysis and screening
- **Perspective**: Individual investor research with AI-assisted analysis
- **Cultural Context**: Japanese financial terminology and reporting standards

### Technical Environment
- **Backend**: Python 3.11+ (data processing and API integration)
- **Frontend**: React 19 + TypeScript + Vite (modern web interface)
- **Data Processing**: yfinance, pandas, Papa Parse
- **Deployment**: Docker + nginx (å€‹äººç’°å¢ƒå‘ã‘)
- **CSV Processing**: Robust handling of Japanese financial data formats
- **Version Control**: Git-based data versioning and change tracking
- **Containerization**: Docker Compose (nginx production + Python data service)

### Automation Strategy
- **Sequential Workflows**: Multi-part data collection to avoid timeouts and rate limits (GitHub Actions)
- **Workflow Orchestration**: Auto-triggered workflow chains for complete automation (7 workflows)
- **Data Pipeline**: Collection â†’ Combination â†’ Local/Docker Use
- **Quality Assurance**: Build verification, health checks, and error handling
- **Monitoring**: Comprehensive logging and artifact retention (30 days)

## Common Operations

### Data Collection Operations

**Full Sequential Collection (All 3795+ companies):**
```bash
# GitHub Actions (Recommended)
# 1. Navigate to Actions â†’ "ğŸ“Š Sequential Stock Fetch - Part 1"
# 2. Click "Run workflow"
# 3. Wait for all 4 parts to complete (automatic chain)
# 4. CSV combination happens automatically

# Expected duration: 6-8 hours total for all parts
```

**Quick Single File Collection:**
```bash
# GitHub Actions
# Navigate to Actions â†’ "ğŸ“Š Stock Data Fetch"
# Select stock file (stocks_1.json - stocks_4.json)
# Click "Run workflow"

# Expected duration: 1-2 hours per file
```

**Local Development:**
```bash
cd stock_list

# Collect data for specific chunk
python sumalize.py stocks_1.json

# Combine latest CSV files
python combine_latest_csv.py

# Check exports
ls -lh Export/*_combined.csv
```

### Web Application Development

**Local Development:**
```bash
cd stock_search

# Install dependencies
npm install

# Start development server with hot reload
npm run dev
# Access: http://localhost:5173

# Build for production
npm run build
# Output: dist/ directory with CSV files included

# Preview production build
npm run preview
# Access: http://localhost:4173

# Manually copy CSV files (normally done in prebuild)
npm run copy-csv
```

**Docker Development:**
```bash
# Build and start both services
docker-compose up --build

# Access frontend
open http://localhost:8080

# Check CSV files are accessible
curl http://localhost:8080/csv/

# View logs
docker-compose logs -f frontend-service

# Stop services
docker-compose down

# Clean volumes and rebuild
docker-compose down -v && docker-compose up --build
```

### CSV Combination & Docker Deployment

**CSV Combination:**
```bash
# GitHub Actions (Automated after Sequential Part 4)
# Navigate to Actions â†’ "ğŸ“‹ CSV Combine & Export"
# Combined CSV files in stock_list/Export/

# Local
cd stock_list
python combine_latest_csv.py
# or with custom date
python combine_latest_csv.py --date 20251006
```

**Docker Deployment:**
```bash
# Build and start both services (data collection + frontend)
docker-compose up --build

# Access application
open http://localhost:8080

# View logs
docker-compose logs -f

# Stop and clean
docker-compose down -v
```

### Stock List Update Operations

```bash
# GitHub Actions (when JPX publishes new listings)
# Navigate to Actions â†’ "ğŸ“‹ Stock List Update"
# Optionally provide reason
# Click "Run workflow"

# Local development
cd stock_list
python get_jp_stocklist.py
python split_stocks.py --input stocks_all.json --size 1000

# Verify splits
ls -lh stocks_*.json
```

## GitHub Actions Workflows

### Workflow Management Strategy

**Two-Tier Automation System:**
1. **Data Collection** (5 workflows)
   - `stock-data-fetch.yml` - Manual single file
   - `stock-fetch-sequential-1/2/3/4.yml` - Automated sequential chain

2. **Data Processing** (2 workflows)
   - `csv-combine-export.yml` - CSV combination
   - `stock-list-update.yml` - Master list updates

### Workflow Execution Best Practices

**For Complete Data Update:**
1. Start `stock-fetch-sequential-1.yml` manually
2. Wait for automatic completion of all 4 parts (6-8 hours)
3. Automatic CSV combination follows
4. CSV files available in stock_list/Export/
5. Deploy locally using Docker if needed

**For Quick Testing:**
1. Run `stock-data-fetch.yml` with `stocks_sample.json`
2. Manually run `csv-combine-export.yml` if needed
3. Test locally with Docker

**For Stock List Updates:**
1. Run `stock-list-update.yml` when JPX updates listings
2. Manually verify stocks_all.json and split files
3. Run sequential collection for new companies

### Error Handling and Monitoring

**Built-in Error Handling:**
- Timeout protection (60-120 minutes per workflow)
- Dependency validation before execution
- Build artifact verification
- Git conflict resolution with rebase
- Comprehensive error logging

**Monitoring Features:**
- Real-time workflow status in Actions tab
- Execution time tracking for performance metrics
- Build artifact retention (30 days) for debugging
- Commit messages with timestamps and completion status
- Health checks for deployed services

## Technical Specifications

### Japanese Language Support
- **Character Encoding**: UTF-8 throughout all components
- **Financial Terminology**: Native Japanese terms with English annotations
- **Geographic Data**: Prefecture-based analysis (éƒ½é“åºœçœŒ)
- **Accounting Standards**: Japan GAAP compliance and tracking
- **Market Classifications**: Japanese exchange-specific categories

### Performance Considerations
- **Data Volume**: 3795+ companies across 23+ industry sectors
- **Processing Time**: ~3-5 seconds per company via yfinance API
- **Sequential Processing**: 120 minutes timeout per workflow part
- **Web Interface**: Optimized for large datasets with pagination and lazy loading
- **Storage**: Efficient CSV compression and Git LFS considerations
- **Caching**:
  - Client-side data caching for improved UX
  - GitHub Actions npm/pip cache for faster builds
  - nginx static asset caching with proper headers
- **Bundle Optimization**: Vendor chunks separated for efficient loading

### Integration Points
- **yfinance API**: Primary data source with rate limiting and retry logic
- **JPX Official Data**: Stock list source with Excel â†’ JSON conversion
- **GitHub Actions**: 7 workflows with sequential orchestration
- **React Components**: Modular design for easy extension and maintenance
- **TypeScript**: Type safety for Japanese financial data structures
- **Tailwind CSS + DaisyUI**: Responsive design system
- **Docker + nginx**: Production-ready containerization with multi-stage builds
- **Papa Parse**: CSV parsing with Japanese character support

### Security and Compliance
- **API Rate Limiting**: Respectful usage of external APIs (yfinance, JPX)
- **Error Handling**: Graceful failure handling without data corruption
- **Version Control**: Complete audit trail of all data changes
- **Access Control**: GitHub repository permissions and workflow security
- **Data Validation**: Input validation and sanitization throughout pipeline
- **Docker Security**: Non-root users, minimal base images, proper permissions
- **nginx Security**: Security headers, CORS policies, XSS protection

## Development Workflow

### Local Development Process
1. **Environment Setup**: Python 3.11+ and Node.js 20+ required
2. **Data Development**: Work in `stock_list/` directory for data processing scripts
3. **Web Development**: Work in `stock_search/` directory for React application
4. **Testing**: Use provided test files and validation scripts
5. **Docker Testing**: Use docker-compose for integration testing
6. **Deployment**: Deploy using Docker Compose

### Docker Production Deployment
1. **Data Collection**: Run sequential workflows via GitHub Actions (6-8 hours)
2. **CSV Combination**: Automatic combination after Part 4
3. **Code Changes**: Modify files in `stock_search/` directory if needed
4. **Build & Deploy**: `docker-compose up --build` (builds both services)
5. **Verification**: Check application at http://localhost:8080
6. **Monitoring**: Use `docker-compose logs -f` for real-time logs

### Complete Data Update & Deployment Workflow
1. **Sequential Collection**: Start Part 1 workflow via GitHub Actions
2. **Automatic Chain**: Parts 2-4 execute automatically (6-8 hours)
3. **CSV Combination**: Automatic combination after Part 4 completion
4. **Local Access**: CSV files available in `stock_list/Export/`
5. **Docker Build**: `docker-compose up --build` for local deployment
6. **Verification**: Access application at http://localhost:8080

## ã‚·ã‚¹ãƒ†ãƒ æ¦‚è¦

å€‹äººå‘ã‘æ—¥æœ¬æ ªå¼åˆ†æãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã€‚GitHub Actionsã«ã‚ˆã‚‹è‡ªå‹•ãƒ‡ãƒ¼ã‚¿åé›†ã¨ã€Docker + nginxã«ã‚ˆã‚‹ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã‚’æä¾›ã€‚

**ä¸»è¦æ©Ÿèƒ½:**
- JPXå…¬å¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã®æ ªå¼ãƒªã‚¹ãƒˆè‡ªå‹•å–å¾—ï¼ˆ3795+éŠ˜æŸ„ï¼‰
- yfinance APIã«ã‚ˆã‚‹è²¡å‹™ãƒ‡ãƒ¼ã‚¿åé›†
- 4æ®µéšSequentialå®Ÿè¡Œã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå›é¿
- React 19 + TypeScript + Viteã«ã‚ˆã‚‹ãƒ¢ãƒ€ãƒ³ãªã‚¦ã‚§ãƒ–ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
- Docker Composeã«ã‚ˆã‚‹2ã‚µãƒ¼ãƒ“ã‚¹æ§‹æˆï¼ˆPython + nginxï¼‰
- æ—¥æœ¬èªè²¡å‹™ãƒ‡ãƒ¼ã‚¿ã®å®Œå…¨ã‚µãƒãƒ¼ãƒˆ

**ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ:**
- **é–‹ç™º**: Vite dev server (port 5173) + Vite preview (port 4173)
- **æœ¬ç•ª**: nginx production server (port 8080) via Docker Compose

è©³ç´°ã¯[DOCKER.md](DOCKER.md)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

## Support / å¯„ä»˜

ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãŒå½¹ç«‹ã¤å ´åˆã¯ã€ã‚µãƒãƒ¼ãƒˆã‚’ã”æ¤œè¨ãã ã•ã„ã€‚

[![GitHub Sponsors](https://img.shields.io/badge/Sponsor-GitHub-pink?style=for-the-badge&logo=github)](https://github.com/sponsors/testkun08080)
[![Buy Me A Coffee](https://img.shields.io/badge/Buy%20Me%20A%20Coffee-Support-yellow?style=for-the-badge&logo=buy-me-a-coffee)](https://www.buymeacoffee.com/testkun08080)

**å¯„ä»˜æ–¹æ³•:**
- **GitHub Sponsors**: ç¶™ç¶šçš„ãªã‚µãƒãƒ¼ãƒˆã€æœˆé¡ãƒ—ãƒ©ãƒ³ã‚ã‚Š
- **Buy Me a Coffee**: ä¸€å›é™ã‚Šã®å¯„ä»˜ã€æ„Ÿè¬ã®æ°—æŒã¡ã‚’ä¼ãˆã‚‹

ã”æ”¯æ´ã„ãŸã ã„ãŸã‚µãƒãƒ¼ãƒˆã¯ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ç¶™ç¶šçš„ãªé–‹ç™ºã¨æ”¹å–„ã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚
